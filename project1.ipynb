{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = np.array([[1,2,3,8,5], [12, 23, 12, 9, 43], [0, 23, 8, 8, 4]])\n",
    "a_3 = a[:, 3]\n",
    "a_without_3 = a[:, np.array(range(a.shape[1])) != 3]\n",
    "\n",
    "a_sep = []\n",
    "for i in np.unique(a[:, 3]):\n",
    "    a_sep.append(a_without_3[a_3 == i, :])\n",
    "a_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.001792107451957\n",
      "0.021304973616162473\n",
      "1.0444020546842818\n",
      "2.0674991357524015\n"
     ]
    }
   ],
   "source": [
    "# JULES FLO: si on sanitize aprÃ¨s il y avait des nan jsp pourquoi, amdahl\n",
    "sanitized_x = sanitize(x)\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "feature_22 = sanitized_x[:, 22]\n",
    "x_minus_22 = sanitized_x[:, np.array(range(x.shape[1])) != 22]\n",
    "x_sep = []\n",
    "y_sep = []\n",
    "\n",
    "for i in np.unique(feature_22):\n",
    "    print(i)\n",
    "    x_sep.append(x_minus_22[feature_22 == i, :])\n",
    "    y_sep.append(y[feature_22 == i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sep[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.38476846,  0.9103791 , -0.00585329, ...,  0.        ,\n",
       "          0.        , -0.74543941],\n",
       "        [ 0.94253641, -0.91455619,  1.31336873, ...,  0.        ,\n",
       "          0.        , -0.74543941],\n",
       "        [-0.28385846,  0.03732318,  0.48512597, ...,  0.        ,\n",
       "          0.        , -0.74543941],\n",
       "        ...,\n",
       "        [ 0.        ,  0.2529135 , -0.32082851, ...,  0.        ,\n",
       "          0.        , -0.74543941],\n",
       "        [-0.46960659, -0.84532397, -0.30297338, ...,  0.        ,\n",
       "          0.        , -0.74543941],\n",
       "        [ 0.        ,  0.66533608, -0.25352276, ...,  0.        ,\n",
       "          0.        , -0.74543941]]),\n",
       " array([[ 0.68202131,  0.55250482,  0.54013641, ...,  0.        ,\n",
       "          0.        , -0.27381996],\n",
       "        [ 0.        ,  3.19515553,  1.09655998, ...,  0.        ,\n",
       "          0.        , -0.29396985],\n",
       "        [ 0.57693915, -1.09837382,  0.33143472, ...,  0.        ,\n",
       "          0.        , -0.43285609],\n",
       "        ...,\n",
       "        [ 0.        ,  0.8209459 , -0.03632212, ...,  0.        ,\n",
       "          0.        ,  0.0603926 ],\n",
       "        [ 0.2024236 ,  0.80068833,  0.19121441, ...,  0.        ,\n",
       "          0.        , -0.02138021],\n",
       "        [-0.28624947,  0.31931645, -0.13086367, ...,  0.        ,\n",
       "          0.        , -0.31701723]]),\n",
       " array([[ 0.28991353,  0.06833197,  0.40768027, ...,  0.61614788,\n",
       "         -1.36131161,  0.4125105 ],\n",
       "        [ 0.46939617, -0.57654339,  0.6515044 , ...,  0.07030726,\n",
       "         -1.52202162,  1.08975056],\n",
       "        [ 0.34246334, -1.37230367,  0.7445519 , ..., -0.38693885,\n",
       "         -1.5319284 ,  2.09093949],\n",
       "        ...,\n",
       "        [-0.03358804, -0.8250664 ,  0.18538516, ..., -0.84320058,\n",
       "         -1.51596748,  0.40680732],\n",
       "        [ 0.07491483, -0.57198826,  0.34475871, ..., -0.2889927 ,\n",
       "         -1.38883045,  1.22464764],\n",
       "        [ 1.66081601, -0.05895686, -0.44963525, ..., -0.02271698,\n",
       "         -0.62490751,  1.03158627]]),\n",
       " array([[-5.60482326e-01, -1.00976110e+00, -5.39645661e-01, ...,\n",
       "          1.16081087e-01,  1.71034105e+00,  1.23037122e+00],\n",
       "        [-1.24167086e-01, -1.10210845e+00, -1.33974234e-01, ...,\n",
       "          8.78486067e-01, -1.14336242e+00,  9.44497970e-01],\n",
       "        [-3.09833316e-03, -1.22221093e+00, -1.95450735e-01, ...,\n",
       "         -6.12854856e-01, -3.34308588e-01,  1.84048922e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  9.79809367e-01, -1.57000161e+00, ...,\n",
       "          1.51685873e+00,  8.88078183e-01,  2.02792903e+00],\n",
       "        [ 0.00000000e+00, -3.15656463e-01,  1.63803773e+01, ...,\n",
       "          2.59308879e-01,  2.29827585e-01,  1.33146751e+00],\n",
       "        [ 1.43398872e-01, -1.28227632e+00, -3.52717911e-01, ...,\n",
       "          2.90316957e-01, -1.21821366e+00,  4.82578340e+00]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 5.172329698511052 and gradient norm: 5.034999302187731\n",
      "Iteration : 250 with loss 0.31697327454101987 and gradient norm: 0.203492622292176\n",
      "Iteration : 500 with loss 0.2811813128078066 and gradient norm: 0.05816661615871858\n",
      "Iteration : 750 with loss 0.27709930561786345 and gradient norm: 0.02813249572218435\n"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(x_sep[0].shape[1], 1)\n",
    "w, loss = least_squares_GD(y_sep[0], x_sep[0], w_init, 1000, 0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62717446]\n",
      " [-0.351299  ]\n",
      " [-0.38718724]\n",
      " ...\n",
      " [-0.66857011]\n",
      " [ 0.09379569]\n",
      " [-0.53285079]]\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "0.8168006165363867\n"
     ]
    }
   ],
   "source": [
    "prediction = x_sep[0] @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "# JULES FLO: 81% accuracy sur le premier putain de merde: LE PLATEAU D'ARGENT\n",
    "print(np.sum(prediction == y_sep[0])/y_sep[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_linear(y, x, w):\n",
    "    return np.sum(x @ w == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 4.937772384969537 and gradient norm: 4.228983316573297\n",
      "Iteration : 250 with loss 0.30602597127278447 and gradient norm: 0.18450464131426314\n",
      "Iteration : 500 with loss 0.2784152240948924 and gradient norm: 0.05108569068152488\n",
      "Iteration : 750 with loss 0.2751397856513302 and gradient norm: 0.025284386070332822\n",
      "0 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 6.083274826142171 and gradient norm: 5.10742363445162\n",
      "Iteration : 250 with loss 0.42072091376084875 and gradient norm: 0.17261766310792043\n",
      "Iteration : 500 with loss 0.3956973703869634 and gradient norm: 0.05214486448993269\n",
      "Iteration : 750 with loss 0.39161957810698084 and gradient norm: 0.032763200935022975\n",
      "1 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 9.711191934803637 and gradient norm: 8.530077749262606\n",
      "Iteration : 250 with loss 0.44595043035616466 and gradient norm: 0.2730022938015295\n",
      "Iteration : 500 with loss 0.3765558422552624 and gradient norm: 0.09358339136680158\n",
      "Iteration : 750 with loss 0.3642885858092467 and gradient norm: 0.05196319863831991\n",
      "2 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 28.672361321543182 and gradient norm: 31.818016410039238\n",
      "Iteration : 250 with loss 0.45435004006328683 and gradient norm: 0.24402524767876618\n",
      "Iteration : 500 with loss 0.39124668640143556 and gradient norm: 0.09863181186808943\n",
      "Iteration : 750 with loss 0.37682648751007725 and gradient norm: 0.05799390125300059\n",
      "3 :\n",
      "Accuracy on training set 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_sep)):\n",
    "    tx = x_sep[i]\n",
    "    ty = y_sep[i]\n",
    "    w, loss = least_squares_GD(ty, tx, w_init, 1000, 0.01, verbose=True)\n",
    "    print(i,\":\")\n",
    "    print(\"Accuracy on training set\", accuracy_linear(ty, tx, w)) # JULES FLO: accuracy 0, bad dans mon calcul, pas le temps de fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss -1440136266.2580655 and gradient norm: 1.4853190575334942\n",
      "Iteration : 250 with loss -252612012443.8168 and gradient norm: 0.9956756381694294\n",
      "Iteration : 500 with loss -503765650455.6133 and gradient norm: 0.9956756337697791\n",
      "Iteration : 750 with loss -754919535292.5062 and gradient norm: 0.9956756319783632\n",
      "0 :\n",
      "Accuracy on training set 0.7541060722828861\n",
      "Iteration : 0 with loss -144833916.71591997 and gradient norm: 0.6946822409016756\n",
      "Iteration : 250 with loss -9868258136.797886 and gradient norm: 0.2076441276726648\n",
      "Iteration : 500 with loss -19625684870.420197 and gradient norm: 0.20764412015006836\n",
      "Iteration : 750 with loss -29383313773.395008 and gradient norm: 0.2076441197989506\n",
      "1 :\n",
      "Accuracy on training set 0.6933482925822758\n",
      "Iteration : 0 with loss -55361760.323068276 and gradient norm: 1.2373732212015471\n",
      "Iteration : 250 with loss -7565402387.639501 and gradient norm: 0.2645289558469356\n",
      "Iteration : 500 with loss -15018527388.389538 and gradient norm: 0.26452867711711964\n",
      "Iteration : 750 with loss -22472892273.940884 and gradient norm: 0.26452867373918193\n",
      "2 :\n",
      "Accuracy on training set 0.676075348855674\n",
      "Iteration : 0 with loss -361095458.9655106 and gradient norm: 4.931514106811956\n",
      "Iteration : 250 with loss -29154285711.59607 and gradient norm: 1.529622810150531\n",
      "Iteration : 500 with loss -57947010117.71623 and gradient norm: 1.5296226967049347\n",
      "Iteration : 750 with loss -86739715532.74715 and gradient norm: 1.5296226967049347\n",
      "3 :\n",
      "Accuracy on training set 0.6965349214943151\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_sep)):\n",
    "    tx = x_sep[i]\n",
    "    ty = y_sep[i]\n",
    "    w, loss = logistic_regression(ty, tx, w_init, 1000, 0.1, verbose=True)\n",
    "    print(i,\":\")\n",
    "    print(\"Accuracy on training set\", compute_accuracy(ty, tx, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.random.rand(sanitized_x.shape[1], 1)\n",
    "print(w_init.shape)\n",
    "w, loss = least_squares_GD(sanitized_y, sanitized_x, np.random.rand(sanitized_x.shape[1], 1), 1000, 0.01, verbose=True)\n",
    "print(w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sanitized_x @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "print(np.sum(prediction == sanitized_y)/sanitized_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(sanitized_y, sanitized_x, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensions and standardize the columns of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running logistic regression with different values of degrees (polynomial feature expansion), gamma and different initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, tx_train.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x[:, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "sanitized_x[:, 22]\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(30)\n",
    "np.around(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
