{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = np.array([[1,2,3,8,5], [12, 23, 12, 9, 43], [0, 23, 8, 8, 4]])\n",
    "a_3 = a[:, 3]\n",
    "a_without_3 = a[:, np.array(range(a.shape[1])) != 3]\n",
    "\n",
    "a_sep = []\n",
    "for i in np.unique(a[:, 3]):\n",
    "    a_sep.append(a_without_3[a_3 == i, :])\n",
    "a_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_21 = x[:, 21]\n",
    "x_minus_21 = x[:, np.array(range(x.shape[1])) != 21]\n",
    "x_sep = []\n",
    "for i in np.unique(feature_21):\n",
    "    x_sep.append(x_minus_21[feature_21 == i, :])\n",
    "\n",
    "for i, x_cat in enumerate(x_sep):\n",
    "    x_sep[i] = sanitize(x_cat)\n",
    "    \n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanitized_y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x_cat in enumerate(x_sep):\n",
    "    x_sep[i] = sanitize(x_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "print_info() missing 1 required positional argument: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8995b0fe69d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, iters, gamma, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mprint_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: print_info() missing 1 required positional argument: 'N'"
     ]
    }
   ],
   "source": [
    "w_init = np.random.rand(sanitized_x.shape[1], 1)\n",
    "print(w_init.shape)\n",
    "w, loss = least_squares_GD(sanitized_y, sanitized_x, np.random.rand(sanitized_x.shape[1], 1), 1000, 0.01, verbose=True)\n",
    "print(w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25516513]\n",
      " [ 0.31391931]\n",
      " [-0.42809016]\n",
      " ...\n",
      " [-0.0349503 ]\n",
      " [ 0.41117694]\n",
      " [-0.33713466]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "0.71338\n"
     ]
    }
   ],
   "source": [
    "prediction = sanitized_x @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "print(np.sum(prediction == sanitized_y)/sanitized_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(sanitized_y, sanitized_x, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensions and standardize the columns of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running logistic regression with different values of degrees (polynomial feature expansion), gamma and different initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d42fb3f34ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, tx_train.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running logistic regression with polynomial feature expansion with degree 1 and gamma 0.01 , with 1 different initial weights.\n",
      "grad norm: 199306.688498 with shape (31, 1)\n",
      "Iteration : 0 with loss -145659968.18611288 and gradient norm: 0.996533442488351\n",
      "grad norm: 107193.921865 with shape (31, 1)\n",
      "Iteration : 250 with loss -31061156022.882534 and gradient norm: 0.5359696093259502\n",
      "grad norm: 107193.918952 with shape (31, 1)\n",
      "Iteration : 500 with loss -61890059878.312744 and gradient norm: 0.5359695947615115\n",
      "grad norm: 107193.91872 with shape (31, 1)\n",
      "Iteration : 750 with loss -92719347883.98398 and gradient norm: 0.5359695935982532\n",
      "(200000, 1) (200000, 31) (31, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.70926\n",
      "Accuracy on test set:,  0.70512\n",
      "Running logistic regression with polynomial feature expansion with degree 1 and gamma 0.05 , with 1 different initial weights.\n",
      "grad norm: 202614.604895 with shape (31, 1)\n",
      "Iteration : 0 with loss -816451996.3234181 and gradient norm: 1.0130730244748851\n",
      "grad norm: 107193.922614 with shape (31, 1)\n",
      "Iteration : 250 with loss -155315987978.32376 and gradient norm: 0.5359696130696101\n",
      "grad norm: 107193.919226 with shape (31, 1)\n",
      "Iteration : 500 with loss -309459544517.0665 and gradient norm: 0.5359695961275892\n",
      "grad norm: 107193.918967 with shape (31, 1)\n",
      "Iteration : 750 with loss -463605041956.1201 and gradient norm: 0.535969594833812\n",
      "(200000, 1) (200000, 31) (31, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.70926\n",
      "Accuracy on test set:,  0.70512\n",
      "Running logistic regression with polynomial feature expansion with degree 1 and gamma 0.1 , with 1 different initial weights.\n",
      "grad norm: 193108.657044 with shape (31, 1)\n",
      "Iteration : 0 with loss -1613365866.5772016 and gradient norm: 0.9655432852207348\n",
      "grad norm: 107193.921129 with shape (31, 1)\n",
      "Iteration : 250 with loss -310562999687.51965 and gradient norm: 0.5359696056431241\n",
      "grad norm: 107193.919553 with shape (31, 1)\n",
      "Iteration : 500 with loss -618848921943.411 and gradient norm: 0.5359695977647205\n",
      "grad norm: 107193.919018 with shape (31, 1)\n",
      "Iteration : 750 with loss -927139254908.4161 and gradient norm: 0.5359695950880374\n",
      "(200000, 1) (200000, 31) (31, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.709265\n",
      "Accuracy on test set:,  0.70514\n",
      "Running logistic regression with polynomial feature expansion with degree 1 and gamma 0.5 , with 1 different initial weights.\n",
      "grad norm: 211758.671191 with shape (31, 1)\n",
      "Iteration : 0 with loss -9656754449.5136 and gradient norm: 1.058793355956098\n",
      "grad norm: 107193.923825 with shape (31, 1)\n",
      "Iteration : 250 with loss -1553547660589.4617 and gradient norm: 0.5359696191269174\n",
      "grad norm: 107193.919681 with shape (31, 1)\n",
      "Iteration : 500 with loss -3094976855210.4165 and gradient norm: 0.5359695984027147\n",
      "grad norm: 107193.919013 with shape (31, 1)\n",
      "Iteration : 750 with loss -4636431850980.017 and gradient norm: 0.5359695950670209\n",
      "(200000, 1) (200000, 31) (31, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.709275\n",
      "Accuracy on test set:,  0.7051\n",
      "Running logistic regression with polynomial feature expansion with degree 2 and gamma 0.01 , with 1 different initial weights.\n",
      "grad norm: 1232311.38742 with shape (61, 1)\n",
      "Iteration : 0 with loss -4029625994.048119 and gradient norm: 6.161556937119977\n",
      "grad norm: 370409.586478 with shape (61, 1)\n",
      "Iteration : 250 with loss -347304646425.8341 and gradient norm: 1.852047932388386\n",
      "grad norm: 370409.547849 with shape (61, 1)\n",
      "Iteration : 500 with loss -690549893935.136 and gradient norm: 1.8520477392446002\n",
      "grad norm: 370409.542393 with shape (61, 1)\n",
      "Iteration : 750 with loss -1033794034599.1261 and gradient norm: 1.8520477119668437\n",
      "(200000, 1) (200000, 61) (61, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.65951\n",
      "Accuracy on test set:,  0.65722\n",
      "Running logistic regression with polynomial feature expansion with degree 2 and gamma 0.05 , with 1 different initial weights.\n",
      "grad norm: 1238271.67935 with shape (61, 1)\n",
      "Iteration : 0 with loss -20250966487.992958 and gradient norm: 6.1913583967535155\n",
      "grad norm: 370409.586478 with shape (61, 1)\n",
      "Iteration : 250 with loss -1736626631198.371 and gradient norm: 1.852047932388386\n",
      "grad norm: 370409.547849 with shape (61, 1)\n",
      "Iteration : 500 with loss -3452852902911.8545 and gradient norm: 1.8520477392446002\n",
      "grad norm: 370409.542393 with shape (61, 1)\n",
      "Iteration : 750 with loss -5169073743349.537 and gradient norm: 1.8520477119668437\n",
      "(200000, 1) (200000, 61) (61, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.65951\n",
      "Accuracy on test set:,  0.65722\n",
      "Running logistic regression with polynomial feature expansion with degree 2 and gamma 0.1 , with 1 different initial weights.\n",
      "grad norm: 1237537.97939 with shape (61, 1)\n",
      "Iteration : 0 with loss -40454815759.28688 and gradient norm: 6.187689896958312\n",
      "grad norm: 370409.586478 with shape (61, 1)\n",
      "Iteration : 250 with loss -3473206582012.366 and gradient norm: 1.852047932388386\n",
      "grad norm: 370409.547849 with shape (61, 1)\n",
      "Iteration : 500 with loss -6905659144440.659 and gradient norm: 1.8520477392446002\n",
      "grad norm: 370409.542393 with shape (61, 1)\n",
      "Iteration : 750 with loss -10338100927064.26 and gradient norm: 1.8520477119668437\n",
      "(200000, 1) (200000, 61) (61, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.65951\n",
      "Accuracy on test set:,  0.65722\n",
      "Running logistic regression with polynomial feature expansion with degree 2 and gamma 0.5 , with 1 different initial weights.\n",
      "grad norm: 1232439.79619 with shape (61, 1)\n",
      "Iteration : 0 with loss -201746158780.44003 and gradient norm: 6.162198980946628\n",
      "grad norm: 370409.586478 with shape (61, 1)\n",
      "Iteration : 250 with loss -17365495038715.809 and gradient norm: 1.852047932388386\n",
      "grad norm: 370409.547849 with shape (61, 1)\n",
      "Iteration : 500 with loss -34527756713069.64 and gradient norm: 1.8520477392446002\n",
      "grad norm: 370409.542393 with shape (61, 1)\n",
      "Iteration : 750 with loss -51689963404927.41 and gradient norm: 1.8520477119668437\n",
      "(200000, 1) (200000, 61) (61, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.65951\n",
      "Accuracy on test set:,  0.65722\n",
      "Running logistic regression with polynomial feature expansion with degree 3 and gamma 0.01 , with 1 different initial weights.\n",
      "grad norm: 3038271.13115 with shape (91, 1)\n",
      "Iteration : 0 with loss -12413497219.353317 and gradient norm: 15.19135565573316\n",
      "grad norm: 1095508.42203 with shape (91, 1)\n",
      "Iteration : 250 with loss -3033039159915.366 and gradient norm: 5.477542110151179\n",
      "grad norm: 1094635.33288 with shape (91, 1)\n",
      "Iteration : 500 with loss -5971160408962.969 and gradient norm: 5.47317666439538\n",
      "grad norm: 1094761.44715 with shape (91, 1)\n",
      "Iteration : 750 with loss -8952914959757.188 and gradient norm: 5.4738072357522345\n",
      "(200000, 1) (200000, 91) (91, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.646555\n",
      "Accuracy on test set:,  0.64382\n",
      "Running logistic regression with polynomial feature expansion with degree 3 and gamma 0.05 , with 1 different initial weights.\n",
      "grad norm: 3041988.50487 with shape (91, 1)\n",
      "Iteration : 0 with loss -62664202308.29691 and gradient norm: 15.20994252435429\n",
      "grad norm: 1095508.70396 with shape (91, 1)\n",
      "Iteration : 250 with loss -15165583115058.441 and gradient norm: 5.477543519799931\n",
      "grad norm: 1094635.33288 with shape (91, 1)\n",
      "Iteration : 500 with loss -29856188120565.32 and gradient norm: 5.47317666439538\n",
      "grad norm: 1094761.44715 with shape (91, 1)\n",
      "Iteration : 750 with loss -44764948162285.8 and gradient norm: 5.4738072357522345\n",
      "(200000, 1) (200000, 91) (91, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.646565\n",
      "Accuracy on test set:,  0.64382\n",
      "Running logistic regression with polynomial feature expansion with degree 3 and gamma 0.1 , with 1 different initial weights.\n",
      "grad norm: 3031484.81207 with shape (91, 1)\n",
      "Iteration : 0 with loss -122161103024.1775 and gradient norm: 15.157424060361567\n",
      "grad norm: 1095508.20238 with shape (91, 1)\n",
      "Iteration : 250 with loss -30329500501379.65 and gradient norm: 5.477541011899232\n",
      "grad norm: 1094635.36799 with shape (91, 1)\n",
      "Iteration : 500 with loss -59710689993170.78 and gradient norm: 5.473176839931624\n",
      "grad norm: 1094761.44715 with shape (91, 1)\n",
      "Iteration : 750 with loss -89528234157220.44 and gradient norm: 5.4738072357522345\n",
      "(200000, 1) (200000, 91) (91, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.646555\n",
      "Accuracy on test set:,  0.64382\n",
      "Running logistic regression with polynomial feature expansion with degree 3 and gamma 0.5 , with 1 different initial weights.\n",
      "grad norm: 3034606.85884 with shape (91, 1)\n",
      "Iteration : 0 with loss -615663122430.8754 and gradient norm: 15.173034294198999\n",
      "grad norm: 1095508.26356 with shape (91, 1)\n",
      "Iteration : 250 with loss -151649462559369.2 and gradient norm: 5.477541317804458\n",
      "grad norm: 1094635.37599 with shape (91, 1)\n",
      "Iteration : 500 with loss -298555436402078.44 and gradient norm: 5.473176879965819\n",
      "grad norm: 1094761.44715 with shape (91, 1)\n",
      "Iteration : 750 with loss -447643018503080.3 and gradient norm: 5.4738072357522345\n",
      "(200000, 1) (200000, 91) (91, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.646555\n",
      "Accuracy on test set:,  0.64382\n",
      "Running logistic regression with polynomial feature expansion with degree 4 and gamma 0.01 , with 1 different initial weights.\n",
      "grad norm: 24024974.2285 with shape (121, 1)\n",
      "Iteration : 0 with loss 10781160948176.646 and gradient norm: 120.12487114256622\n",
      "grad norm: 9672729.56917 with shape (121, 1)\n",
      "Iteration : 250 with loss -343761067363841.75 and gradient norm: 48.363647845846835\n",
      "grad norm: 9603871.48825 with shape (121, 1)\n",
      "Iteration : 500 with loss -570094175881466.1 and gradient norm: 48.019357441252396\n",
      "grad norm: 9666485.33293 with shape (121, 1)\n",
      "Iteration : 750 with loss -989455550388298.2 and gradient norm: 48.33242666464704\n",
      "(200000, 1) (200000, 121) (121, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.66386\n",
      "Accuracy on test set:,  0.66196\n",
      "Running logistic regression with polynomial feature expansion with degree 4 and gamma 0.05 , with 1 different initial weights.\n",
      "grad norm: 24025437.4492 with shape (121, 1)\n",
      "Iteration : 0 with loss 53907238584908.125 and gradient norm: 120.12718724595912\n",
      "grad norm: 9672729.56917 with shape (121, 1)\n",
      "Iteration : 250 with loss -1718800464331472.0 and gradient norm: 48.363647845846835\n",
      "grad norm: 9603871.48825 with shape (121, 1)\n",
      "Iteration : 500 with loss -2850502094092821.0 and gradient norm: 48.019357441252396\n",
      "grad norm: 9666485.33293 with shape (121, 1)\n",
      "Iteration : 750 with loss -4947276700355588.0 and gradient norm: 48.33242666464704\n",
      "(200000, 1) (200000, 121) (121, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.66386\n",
      "Accuracy on test set:,  0.66196\n",
      "Running logistic regression with polynomial feature expansion with degree 4 and gamma 0.1 , with 1 different initial weights.\n",
      "grad norm: 24007639.752 with shape (121, 1)\n",
      "Iteration : 0 with loss 107695814711994.66 and gradient norm: 120.03819875990476\n",
      "grad norm: 9672729.56917 with shape (121, 1)\n",
      "Iteration : 250 with loss -3437556933143137.0 and gradient norm: 48.363647845846835\n",
      "grad norm: 9603871.48825 with shape (121, 1)\n",
      "Iteration : 500 with loss -5701007029741739.0 and gradient norm: 48.019357441252396\n",
      "grad norm: 9666485.33293 with shape (121, 1)\n",
      "Iteration : 750 with loss -9894544227454128.0 and gradient norm: 48.33242666464704\n",
      "(200000, 1) (200000, 121) (121, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.66386\n",
      "Accuracy on test set:,  0.66196\n",
      "Running logistic regression with polynomial feature expansion with degree 4 and gamma 0.5 , with 1 different initial weights.\n",
      "grad norm: 24025789.3248 with shape (121, 1)\n",
      "Iteration : 0 with loss 539085111174841.75 and gradient norm: 120.12894662387956\n",
      "grad norm: 9672729.56917 with shape (121, 1)\n",
      "Iteration : 250 with loss -1.718799462917672e+16 and gradient norm: 48.363647845846835\n",
      "grad norm: 9603871.48825 with shape (121, 1)\n",
      "Iteration : 500 with loss -2.850501092664171e+16 and gradient norm: 48.019357441252396\n",
      "grad norm: 9666485.33293 with shape (121, 1)\n",
      "Iteration : 750 with loss -4.947275698952992e+16 and gradient norm: 48.33242666464704\n",
      "(200000, 1) (200000, 121) (121, 1)\n",
      "0 :\n",
      "Accuracy on training set 0.66386\n",
      "Accuracy on test set:,  0.66196\n"
     ]
    }
   ],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x[:, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "sanitized_x[:, 22]\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(30)\n",
    "np.around(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98948792, 0.67934654, 0.34673991, 0.48885077, 0.5348651 ,\n",
       "       0.08436905, 0.02071519, 0.18776387, 0.52188862, 0.85832073,\n",
       "       0.50843456, 0.60499752, 0.84728409, 0.93295583, 0.00486189,\n",
       "       0.66121463, 0.3157777 , 0.6534353 , 0.12888288, 0.54033186,\n",
       "       0.44024959, 0.37952039, 0.0833081 , 0.55721317, 0.36309742,\n",
       "       0.2762448 , 0.46041519, 0.00914704, 0.35695435, 0.45490402])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
