{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' \n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = np.array([[1,2,3,8,5], [12, 23, 12, 9, 43], [0, 23, 8, 8, 4]])\n",
    "a_3 = a[:, 3]\n",
    "a_without_3 = a[:, np.array(range(a.shape[1])) != 3]\n",
    "\n",
    "a_sep = []\n",
    "for i in np.unique(a[:, 3]):\n",
    "    a_sep.append(a_without_3[a_3 == i, :])\n",
    "a_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "sanitized_x = sanitize(x)\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "feature_22 = sanitized_x[:, 22]\n",
    "x_minus_22 = sanitized_x[:, np.array(range(sanitized_x.shape[1])) != 22]\n",
    "x_sep = []\n",
    "y_sep = []\n",
    "\n",
    "for i in np.unique(feature_22):\n",
    "    print(i)\n",
    "    x_sep.append(x_minus_22[feature_22 == i, :])\n",
    "    y_sep.append(y[feature_22 == i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 7.904257110444684 and gradient norm: 5.9145839170478895\n",
      "Iteration : 250 with loss 0.2729126189015163 and gradient norm: 0.006538765436612333\n",
      "Iteration : 500 with loss 0.2723851038093433 and gradient norm: 0.0032261998004240763\n",
      "Iteration : 750 with loss 0.27222545231901113 and gradient norm: 0.0018971405518997612\n",
      "Iteration : 1000 with loss 0.2721693309557731 and gradient norm: 0.001132208637022638\n",
      "Iteration : 1250 with loss 0.27214927823770474 and gradient norm: 0.0006777310436834496\n",
      "Iteration : 1500 with loss 0.27214208772700405 and gradient norm: 0.00040596948359918403\n",
      "Iteration : 1750 with loss 0.27213950720027447 and gradient norm: 0.00024322117567043692\n",
      "Iteration : 2000 with loss 0.2721385809199796 and gradient norm: 0.00014572241540480177\n",
      "Iteration : 2250 with loss 0.27213824841615514 and gradient norm: 8.730826249046025e-05\n",
      "Iteration : 2500 with loss 0.27213812905703216 and gradient norm: 5.2310066820892636e-05\n",
      "Iteration : 2750 with loss 0.27213808621049634 and gradient norm: 3.1341186017601384e-05\n",
      "Iteration : 3000 with loss 0.27213807082979724 and gradient norm: 1.877783990362236e-05\n",
      "Iteration : 3250 with loss 0.2721380653085589 and gradient norm: 1.1250604284952795e-05\n",
      "Iteration : 3500 with loss 0.27213806332658946 and gradient norm: 6.740716635508965e-06\n",
      "Iteration : 3750 with loss 0.27213806261511825 and gradient norm: 4.038650713247704e-06\n",
      "Iteration : 4000 with loss 0.27213806235972 and gradient norm: 2.419727850836584e-06\n",
      "Iteration : 4250 with loss 0.27213806226803916 and gradient norm: 1.4497621795187956e-06\n",
      "Iteration : 4500 with loss 0.2721380622351283 and gradient norm: 8.686144452942608e-07\n",
      "Iteration : 4750 with loss 0.2721380622233143 and gradient norm: 5.204241446881691e-07\n",
      "Iteration : 0 with loss 6.212215948099324 and gradient norm: 5.220388154863248\n",
      "Iteration : 250 with loss 0.38377432044793125 and gradient norm: 0.01320342377626559\n",
      "Iteration : 500 with loss 0.3812179145242702 and gradient norm: 0.008761922354933829\n",
      "Iteration : 750 with loss 0.3795106967356996 and gradient norm: 0.007798389297018504\n",
      "Iteration : 1000 with loss 0.3781471953124391 and gradient norm: 0.006978137496914861\n",
      "Iteration : 1250 with loss 0.3770553137379607 and gradient norm: 0.006244640545655622\n",
      "Iteration : 1500 with loss 0.37618090963289674 and gradient norm: 0.005588251006348398\n",
      "Iteration : 1750 with loss 0.37548066606535935 and gradient norm: 0.005000856342953345\n",
      "Iteration : 2000 with loss 0.3749198943800795 and gradient norm: 0.004475204169071961\n",
      "Iteration : 2250 with loss 0.37447081509118274 and gradient norm: 0.004004804574181684\n",
      "Iteration : 2500 with loss 0.37411118172696983 and gradient norm: 0.0035838498248566505\n",
      "Iteration : 2750 with loss 0.3738231787843852 and gradient norm: 0.0032071426530870008\n",
      "Iteration : 3000 with loss 0.3735925391853062 and gradient norm: 0.0028700320883740423\n",
      "Iteration : 3250 with loss 0.37340783751460116 and gradient norm: 0.0025683560350427057\n",
      "Iteration : 3500 with loss 0.3732599240283765 and gradient norm: 0.0022983898854173714\n",
      "Iteration : 3750 with loss 0.37314147139433357 and gradient norm: 0.00205680053439439\n",
      "Iteration : 4000 with loss 0.3730466117106776 and gradient norm: 0.0018406052276543337\n",
      "Iteration : 4250 with loss 0.37297064582223594 and gradient norm: 0.0016471347354389682\n",
      "Iteration : 4500 with loss 0.37290981053389927 and gradient norm: 0.0014740003972221547\n",
      "Iteration : 4750 with loss 0.3728610921896142 and gradient norm: 0.0013190646303952787\n",
      "Iteration : 0 with loss 11.717424359899077 and gradient norm: 10.64926103220923\n",
      "Iteration : 250 with loss 0.35557301362323207 and gradient norm: 0.0053135886438758885\n",
      "Iteration : 500 with loss 0.355414576341577 and gradient norm: 0.0007804707290806841\n",
      "Iteration : 750 with loss 0.3554099226882254 and gradient norm: 0.0001664300565648811\n",
      "Iteration : 1000 with loss 0.35540970419456897 and gradient norm: 3.676968266817226e-05\n",
      "Iteration : 1250 with loss 0.3554096934970278 and gradient norm: 8.151128862057534e-06\n",
      "Iteration : 1500 with loss 0.355409692971168 and gradient norm: 1.8075485371026024e-06\n",
      "Iteration : 1750 with loss 0.35540969294530805 and gradient norm: 4.0084519609727203e-07\n",
      "Iteration : 2000 with loss 0.35540969294403635 and gradient norm: 8.889308242035856e-08\n",
      "Iteration : 2250 with loss 0.35540969294397373 and gradient norm: 1.9716191214218997e-08\n",
      "Iteration : 2500 with loss 0.3554096929439706 and gradient norm: 4.385988250027551e-09\n",
      "Iteration : 2750 with loss 0.3554096929439705 and gradient norm: 1.0323866044547633e-09\n",
      "Iteration : 3000 with loss 0.3554096929439705 and gradient norm: 4.149626075490417e-10\n",
      "Iteration : 3250 with loss 0.3554096929439705 and gradient norm: 3.581153066515602e-10\n",
      "Iteration : 3500 with loss 0.3554096929439705 and gradient norm: 3.550849284210778e-10\n",
      "Iteration : 3750 with loss 0.35540969294397057 and gradient norm: 3.549352205820525e-10\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "weights = []\n",
    "for i, x_chunk in enumerate(x_sep):\n",
    "    w_init = np.random.rand(x_chunk.shape[1], 1)\n",
    "    w, loss = least_squares_GD(y_sep[i], x_chunk, w_init, 5000, 0.1, verbose=True) \n",
    "    y_pred = predict_labels(w, x_chunk)\n",
    "    accuracies.append(compute_accuracy_linear_reg(y_sep[i], y_pred))\n",
    "    weights.append(w)\n",
    "    \n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62717446]\n",
      " [-0.351299  ]\n",
      " [-0.38718724]\n",
      " ...\n",
      " [-0.66857011]\n",
      " [ 0.09379569]\n",
      " [-0.53285079]]\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "0.8168006165363867\n"
     ]
    }
   ],
   "source": [
    "prediction = x_sep[0] @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "# JULES FLO: 81% accuracy sur le premier putain de merde: LE PLATEAU D'ARGENT\n",
    "print(np.sum(prediction == y_sep[0])/y_sep[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_linear(y, x, w):\n",
    "    return np.sum(x @ w == y)/y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 4.937772384969537 and gradient norm: 4.228983316573297\n",
      "Iteration : 250 with loss 0.30602597127278447 and gradient norm: 0.18450464131426314\n",
      "Iteration : 500 with loss 0.2784152240948924 and gradient norm: 0.05108569068152488\n",
      "Iteration : 750 with loss 0.2751397856513302 and gradient norm: 0.025284386070332822\n",
      "0 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 6.083274826142171 and gradient norm: 5.10742363445162\n",
      "Iteration : 250 with loss 0.42072091376084875 and gradient norm: 0.17261766310792043\n",
      "Iteration : 500 with loss 0.3956973703869634 and gradient norm: 0.05214486448993269\n",
      "Iteration : 750 with loss 0.39161957810698084 and gradient norm: 0.032763200935022975\n",
      "1 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 9.711191934803637 and gradient norm: 8.530077749262606\n",
      "Iteration : 250 with loss 0.44595043035616466 and gradient norm: 0.2730022938015295\n",
      "Iteration : 500 with loss 0.3765558422552624 and gradient norm: 0.09358339136680158\n",
      "Iteration : 750 with loss 0.3642885858092467 and gradient norm: 0.05196319863831991\n",
      "2 :\n",
      "Accuracy on training set 0.0\n",
      "Iteration : 0 with loss 28.672361321543182 and gradient norm: 31.818016410039238\n",
      "Iteration : 250 with loss 0.45435004006328683 and gradient norm: 0.24402524767876618\n",
      "Iteration : 500 with loss 0.39124668640143556 and gradient norm: 0.09863181186808943\n",
      "Iteration : 750 with loss 0.37682648751007725 and gradient norm: 0.05799390125300059\n",
      "3 :\n",
      "Accuracy on training set 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_sep)):\n",
    "    tx = x_sep[i]\n",
    "    ty = y_sep[i]\n",
    "    w, loss = least_squares_GD(ty, tx, w_init, 1000, 0.01, verbose=True)\n",
    "    print(i,\":\")\n",
    "    print(\"Accuracy on training set\", accuracy_linear(ty, tx, w)) # JULES FLO: accuracy 0, bad dans mon calcul, pas le temps de fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss -1440136266.2580655 and gradient norm: 1.4853190575334942\n",
      "Iteration : 250 with loss -252612012443.8168 and gradient norm: 0.9956756381694294\n",
      "Iteration : 500 with loss -503765650455.6133 and gradient norm: 0.9956756337697791\n",
      "Iteration : 750 with loss -754919535292.5062 and gradient norm: 0.9956756319783632\n",
      "0 :\n",
      "Accuracy on training set 0.7541060722828861\n",
      "Iteration : 0 with loss -144833916.71591997 and gradient norm: 0.6946822409016756\n",
      "Iteration : 250 with loss -9868258136.797886 and gradient norm: 0.2076441276726648\n",
      "Iteration : 500 with loss -19625684870.420197 and gradient norm: 0.20764412015006836\n",
      "Iteration : 750 with loss -29383313773.395008 and gradient norm: 0.2076441197989506\n",
      "1 :\n",
      "Accuracy on training set 0.6933482925822758\n",
      "Iteration : 0 with loss -55361760.323068276 and gradient norm: 1.2373732212015471\n",
      "Iteration : 250 with loss -7565402387.639501 and gradient norm: 0.2645289558469356\n",
      "Iteration : 500 with loss -15018527388.389538 and gradient norm: 0.26452867711711964\n",
      "Iteration : 750 with loss -22472892273.940884 and gradient norm: 0.26452867373918193\n",
      "2 :\n",
      "Accuracy on training set 0.676075348855674\n",
      "Iteration : 0 with loss -361095458.9655106 and gradient norm: 4.931514106811956\n",
      "Iteration : 250 with loss -29154285711.59607 and gradient norm: 1.529622810150531\n",
      "Iteration : 500 with loss -57947010117.71623 and gradient norm: 1.5296226967049347\n",
      "Iteration : 750 with loss -86739715532.74715 and gradient norm: 1.5296226967049347\n",
      "3 :\n",
      "Accuracy on training set 0.6965349214943151\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_sep)):\n",
    "    tx = x_sep[i]\n",
    "    ty = y_sep[i]\n",
    "    w, loss = logistic_regression(ty, tx, w_init, 1000, 0.1, verbose=True)\n",
    "    print(i,\":\")\n",
    "    print(\"Accuracy on training set\", compute_accuracy(ty, tx, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = np.random.rand(sanitized_x.shape[1], 1)\n",
    "print(w_init.shape)\n",
    "w, loss = least_squares_GD(sanitized_y, sanitized_x, np.random.rand(sanitized_x.shape[1], 1), 1000, 0.01, verbose=True)\n",
    "print(w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sanitized_x @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "print(np.sum(prediction == sanitized_y)/sanitized_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_accuracy(sanitized_y, sanitized_x, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensions and standardize the columns of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running logistic regression with different values of degrees (polynomial feature expansion), gamma and different initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, tx_train.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x[:, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "sanitized_x[:, 22]\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(30)\n",
    "np.around(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
