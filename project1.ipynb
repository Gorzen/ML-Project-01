{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_train, x_train, _ = load_csv_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanitizing the missing values and standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "sanitized_x = sanitize(x)\n",
    "y = y.reshape((y.shape[0], 1))\n",
    "\n",
    "feature_22 = sanitized_x[:, 22]\n",
    "x_minus_22 = sanitized_x[:, np.array(range(sanitized_x.shape[1])) != 22]\n",
    "x_sep = []\n",
    "y_sep = []\n",
    "\n",
    "for i in np.unique(feature_22):\n",
    "    print(i)\n",
    "    x_sep.append(x_minus_22[feature_22 == i, :])\n",
    "    y_sep.append(y[feature_22 == i, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 7.904257110444684 and gradient norm: 5.9145839170478895\n",
      "Iteration : 250 with loss 0.2729126189015163 and gradient norm: 0.006538765436612333\n",
      "Iteration : 500 with loss 0.2723851038093433 and gradient norm: 0.0032261998004240763\n",
      "Iteration : 750 with loss 0.27222545231901113 and gradient norm: 0.0018971405518997612\n",
      "Iteration : 1000 with loss 0.2721693309557731 and gradient norm: 0.001132208637022638\n",
      "Iteration : 1250 with loss 0.27214927823770474 and gradient norm: 0.0006777310436834496\n",
      "Iteration : 1500 with loss 0.27214208772700405 and gradient norm: 0.00040596948359918403\n",
      "Iteration : 1750 with loss 0.27213950720027447 and gradient norm: 0.00024322117567043692\n",
      "Iteration : 2000 with loss 0.2721385809199796 and gradient norm: 0.00014572241540480177\n",
      "Iteration : 2250 with loss 0.27213824841615514 and gradient norm: 8.730826249046025e-05\n",
      "Iteration : 2500 with loss 0.27213812905703216 and gradient norm: 5.2310066820892636e-05\n",
      "Iteration : 2750 with loss 0.27213808621049634 and gradient norm: 3.1341186017601384e-05\n",
      "Iteration : 3000 with loss 0.27213807082979724 and gradient norm: 1.877783990362236e-05\n",
      "Iteration : 3250 with loss 0.2721380653085589 and gradient norm: 1.1250604284952795e-05\n",
      "Iteration : 3500 with loss 0.27213806332658946 and gradient norm: 6.740716635508965e-06\n",
      "Iteration : 3750 with loss 0.27213806261511825 and gradient norm: 4.038650713247704e-06\n",
      "Iteration : 4000 with loss 0.27213806235972 and gradient norm: 2.419727850836584e-06\n",
      "Iteration : 4250 with loss 0.27213806226803916 and gradient norm: 1.4497621795187956e-06\n",
      "Iteration : 4500 with loss 0.2721380622351283 and gradient norm: 8.686144452942608e-07\n",
      "Iteration : 4750 with loss 0.2721380622233143 and gradient norm: 5.204241446881691e-07\n",
      "Iteration : 0 with loss 6.212215948099324 and gradient norm: 5.220388154863248\n",
      "Iteration : 250 with loss 0.38377432044793125 and gradient norm: 0.01320342377626559\n",
      "Iteration : 500 with loss 0.3812179145242702 and gradient norm: 0.008761922354933829\n",
      "Iteration : 750 with loss 0.3795106967356996 and gradient norm: 0.007798389297018504\n",
      "Iteration : 1000 with loss 0.3781471953124391 and gradient norm: 0.006978137496914861\n",
      "Iteration : 1250 with loss 0.3770553137379607 and gradient norm: 0.006244640545655622\n",
      "Iteration : 1500 with loss 0.37618090963289674 and gradient norm: 0.005588251006348398\n",
      "Iteration : 1750 with loss 0.37548066606535935 and gradient norm: 0.005000856342953345\n",
      "Iteration : 2000 with loss 0.3749198943800795 and gradient norm: 0.004475204169071961\n",
      "Iteration : 2250 with loss 0.37447081509118274 and gradient norm: 0.004004804574181684\n",
      "Iteration : 2500 with loss 0.37411118172696983 and gradient norm: 0.0035838498248566505\n",
      "Iteration : 2750 with loss 0.3738231787843852 and gradient norm: 0.0032071426530870008\n",
      "Iteration : 3000 with loss 0.3735925391853062 and gradient norm: 0.0028700320883740423\n",
      "Iteration : 3250 with loss 0.37340783751460116 and gradient norm: 0.0025683560350427057\n",
      "Iteration : 3500 with loss 0.3732599240283765 and gradient norm: 0.0022983898854173714\n",
      "Iteration : 3750 with loss 0.37314147139433357 and gradient norm: 0.00205680053439439\n",
      "Iteration : 4000 with loss 0.3730466117106776 and gradient norm: 0.0018406052276543337\n",
      "Iteration : 4250 with loss 0.37297064582223594 and gradient norm: 0.0016471347354389682\n",
      "Iteration : 4500 with loss 0.37290981053389927 and gradient norm: 0.0014740003972221547\n",
      "Iteration : 4750 with loss 0.3728610921896142 and gradient norm: 0.0013190646303952787\n",
      "Iteration : 0 with loss 11.717424359899077 and gradient norm: 10.64926103220923\n",
      "Iteration : 250 with loss 0.35557301362323207 and gradient norm: 0.0053135886438758885\n",
      "Iteration : 500 with loss 0.355414576341577 and gradient norm: 0.0007804707290806841\n",
      "Iteration : 750 with loss 0.3554099226882254 and gradient norm: 0.0001664300565648811\n",
      "Iteration : 1000 with loss 0.35540970419456897 and gradient norm: 3.676968266817226e-05\n",
      "Iteration : 1250 with loss 0.3554096934970278 and gradient norm: 8.151128862057534e-06\n",
      "Iteration : 1500 with loss 0.355409692971168 and gradient norm: 1.8075485371026024e-06\n",
      "Iteration : 1750 with loss 0.35540969294530805 and gradient norm: 4.0084519609727203e-07\n",
      "Iteration : 2000 with loss 0.35540969294403635 and gradient norm: 8.889308242035856e-08\n",
      "Iteration : 2250 with loss 0.35540969294397373 and gradient norm: 1.9716191214218997e-08\n",
      "Iteration : 2500 with loss 0.3554096929439706 and gradient norm: 4.385988250027551e-09\n",
      "Iteration : 2750 with loss 0.3554096929439705 and gradient norm: 1.0323866044547633e-09\n",
      "Iteration : 3000 with loss 0.3554096929439705 and gradient norm: 4.149626075490417e-10\n",
      "Iteration : 3250 with loss 0.3554096929439705 and gradient norm: 3.581153066515602e-10\n",
      "Iteration : 3500 with loss 0.3554096929439705 and gradient norm: 3.550849284210778e-10\n",
      "Iteration : 3750 with loss 0.35540969294397057 and gradient norm: 3.549352205820525e-10\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "weights = []\n",
    "for i, x_chunk in enumerate(x_sep):\n",
    "    w_init = np.random.rand(x_chunk.shape[1], 1)\n",
    "    w, loss = least_squares_GD(y_sep[i], x_chunk, w_init, 5000, 0.1, verbose=True) \n",
    "    y_pred = predict_labels(w, x_chunk)\n",
    "    accuracies.append(compute_accuracy_linear_reg(y_sep[i], y_pred))\n",
    "    weights.append(w)\n",
    "    \n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62717446]\n",
      " [-0.351299  ]\n",
      " [-0.38718724]\n",
      " ...\n",
      " [-0.66857011]\n",
      " [ 0.09379569]\n",
      " [-0.53285079]]\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "0.8168006165363867\n"
     ]
    }
   ],
   "source": [
    "prediction = x_sep[0] @ w\n",
    "print(prediction)\n",
    "\n",
    "prediction[prediction >= 0] = 1\n",
    "prediction[prediction <= 0] = -1\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "# JULES FLO: 81% accuracy sur le premier putain de merde: LE PLATEAU D'ARGENT\n",
    "print(np.sum(prediction == y_sep[0])/y_sep[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_22_train = x_train_san[:, 22]\n",
    "categories_22 = np.unique(feature_22_train)\n",
    "x_train_san_minus_22 = x_train_san[:, np.array(range(x.shape[1])) != 22]\n",
    "x_train_sep = []\n",
    "y_train_sep = []\n",
    "\n",
    "for i in categories_22: # Assuming same set of category in the training and testing\n",
    "    x_train_sep.append(x_train_san_minus_22[feature_22_train == i, :])\n",
    "    y_train_sep.append(y_train[feature_22_train == i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-0c184010f8fd>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-0c184010f8fd>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    predictions[ind] =\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Rebuilding final y for submission, in progress\n",
    "N = x_test.shape[0]\n",
    "predictions = np.zeros(N)\n",
    "for i in categories_22:\n",
    "    ind = np.arange(N)[np.isclose(i, features_22_test, atol=1e-04)]\n",
    "    predictions[ind] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 with loss 6.450879851498423 and gradient norm: 5.2949655558829996\n",
      "Iteration : 250 with loss 0.33888191557092795 and gradient norm: 0.23865532916917823\n",
      "Iteration : 500 with loss 0.28938969362720307 and gradient norm: 0.06947344049309992\n",
      "Iteration : 750 with loss 0.28333925216863215 and gradient norm: 0.03555978862893712\n",
      "Iteration : 0 with loss 7.452108683713229 and gradient norm: 5.818346417752108\n",
      "Iteration : 250 with loss 0.44345963076205164 and gradient norm: 0.21575812346784867\n",
      "Iteration : 500 with loss 0.4012819407366933 and gradient norm: 0.0727687613696616\n",
      "Iteration : 750 with loss 0.3930971455255257 and gradient norm: 0.046254790474539606\n",
      "Iteration : 0 with loss 13.67910152265491 and gradient norm: 12.796020211909392\n",
      "Iteration : 250 with loss 0.5159878990640682 and gradient norm: 0.3421576311697902\n",
      "Iteration : 500 with loss 0.391543583051741 and gradient norm: 0.1334200865209421\n",
      "Iteration : 750 with loss 0.3675070379449254 and gradient norm: 0.06923134444376204\n",
      "Iteration : 0 with loss 47.063741454514876 and gradient norm: 42.507412281174965\n",
      "Iteration : 250 with loss 0.4279531192977199 and gradient norm: 0.2415670237034154\n",
      "Iteration : 500 with loss 0.3737766858484782 and gradient norm: 0.07978364641943766\n",
      "Iteration : 750 with loss 0.3660322870330254 and gradient norm: 0.03599448926534579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8136979171879535,\n",
       " 0.7054317548746518,\n",
       " 0.7315746640465273,\n",
       " 0.7268092402093485]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = []\n",
    "weights = []\n",
    "for i, x_chunk in enumerate(x_train_sep):\n",
    "    w_init = np.random.rand(x_chunk.shape[1], 1)\n",
    "    w, loss = least_squares_GD(y_train_sep[i], x_chunk, w_init, 1000, 0.01, verbose=True) \n",
    "    y_pred = predict_labels(w, x_chunk)\n",
    "    accuracies.append(compute_accuracy_linear_reg(y_train_sep[i], y_pred))\n",
    "    weights.append(w)\n",
    "    \n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old but kept because can be useful as basis\n",
    "N = sanitized_x.shape[0]\n",
    "inds = np.random.choice(range(N), 100000, replace=False)\n",
    "sanitized_x_sub = sanitized_x[inds, :]\n",
    "sanitized_x_sub = sanitized_x_sub[:, [0,1,2,3,7,8,9,10,11,13,14,17,19,21,22,29]]\n",
    "sanitized_y_sub = sanitized_y[inds]\n",
    "x_train, x_test, y_train, y_test = split_data(sanitized_x, sanitized_y, 0.8)\n",
    "number_of_w = 1\n",
    "\n",
    "for degree in range(1, 5)\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    for gamma in np.array([0.01, 0.05, 0.1, 0.5]):\n",
    "        print(\"Running logistic regression with polynomial feature expansion with degree\", degree, \n",
    "                 \"and gamma\", gamma,\", with\", number_of_w, \"different initial weights.\")\n",
    "        for i in  range(number_of_w):\n",
    "            w_init = np.random.rand(tx_train.shape[1], 1)\n",
    "            w, loss = logistic_regression(y_train, tx_train, w_init, 1000, gamma, verbose=True)\n",
    "            print(i,\":\")\n",
    "            print(\"Accuracy on training set\", compute_accuracy(y_train, tx_train, w))\n",
    "            print(\"Accuracy on test set:, \", compute_accuracy(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_, x_test, ids_test = load_csv_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_san  = sanitize(x_test)\n",
    "\n",
    "feature_22_test = x_test_san[:, 22]\n",
    "x_test_san_minus_22 = x_test_san[:, np.array(range(x.shape[1])) != 22]\n",
    "x_test_sep = []\n",
    "\n",
    "for i in categories_22:\n",
    "    x_test_sep.append(x_test_san_minus_22[feature_22_test == i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, x_test)\n",
    "# Rebuild original form for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
